{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout, Dense,Input,Embedding,Flatten, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from textaugment import EDA\n",
    "columns = ['学号', '性别', '生源地', '总分', '幻觉、妄想症状', '自杀意图', '焦虑指标总分', '抑郁指标总分', '偏执指标总分', '自卑指标总分',\n",
    "               '敏感指标总分', '社交恐惧指标总分', '躯体化指标总分', '依赖指标总分', '敌对攻击指标总分', '冲动指标总分', '强迫指标总分',\n",
    "               '网络成瘾指标总分', '自伤行为指标总分', '进食问题指标总分', '睡眠困扰指标总分', '学校适应困难指标总分', '人际关系困扰指标总分',\n",
    "               '学业压力指标总分', '就业压力指标总分', '恋爱困扰指标总分']\n",
    "data = pd.read_csv('student_data.csv', encoding='utf-8')\n",
    "data.drop(columns=columns, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "0    106\n1     72\nName: 可能问题, dtype: int64"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['可能问题'] != 0, '可能问题'] = 1\n",
    "data['可能问题'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=8)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "178"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = np.array(pd.concat([train_df['text'], test_df['text']]))\n",
    "len(texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\keras_preprocessing\\text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "data": {
      "text/plain": "178"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5000\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "len(sequences)\n",
    "# sequences即将每个句子中的每个单词使用词典序表示的形式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6375 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "labels = to_categorical(np.array(pd.concat([train_df['可能问题'], test_df['可能问题']])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (178, 500)\n",
      "Shape of label tensor: (178, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences in X_train:  142\n",
      "number of sequences in X_test:  36\n",
      "142\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "X_train = data[:len(train_df)]\n",
    "X_test = data[len(train_df):]\n",
    "y_train = np.array(train_df['可能问题'])\n",
    "y_test = np.array(test_df['可能问题'])\n",
    "\n",
    "print('number of sequences in X_train: ', len(X_train))\n",
    "print('number of sequences in X_test: ', len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Preparing the Embedding Layer\n",
    "embedding_index = {}\n",
    "f = open('./glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embedding_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# embedding matrix\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[index]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[index])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[index] = embedding_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "metadata_input = Input(shape=(train_metadata.shape[1],), dtype='float32')\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_model = Dropout(0.5)(embedded_sequences)\n",
    "l_model = Bidirectional(keras.layers.LSTM(128, recurrent_dropout=0.2))(l_model)\n",
    "l_model = Dropout(0.5)(l_model)\n",
    "\n",
    "c_dense = keras.layers.Dense(1024, activation='relu')(l_model)\n",
    "preds = keras.layers.Dense(4)(c_dense)\n",
    "preds = keras.layers.Activation('softmax')(preds)\n",
    "\n",
    "model = Model([sequence_input, metadata_input], preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, \"LSTM_metadata_model.png\", show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "142/142 - 31s - loss: 1.3311 - accuracy: 0.3239 - val_loss: 0.9499 - val_accuracy: 0.6111\n",
      "Epoch 2/20\n",
      "142/142 - 28s - loss: 0.9571 - accuracy: 0.6056 - val_loss: 0.7453 - val_accuracy: 0.6111\n",
      "Epoch 3/20\n",
      "142/142 - 28s - loss: 0.7578 - accuracy: 0.5845 - val_loss: 0.6871 - val_accuracy: 0.6111\n",
      "Epoch 4/20\n",
      "142/142 - 31s - loss: 0.6959 - accuracy: 0.5915 - val_loss: 0.6890 - val_accuracy: 0.5556\n",
      "Epoch 5/20\n",
      "142/142 - 31s - loss: 0.6821 - accuracy: 0.5775 - val_loss: 0.6862 - val_accuracy: 0.5556\n",
      "Epoch 6/20\n",
      "142/142 - 30s - loss: 0.6559 - accuracy: 0.6268 - val_loss: 0.6796 - val_accuracy: 0.6111\n",
      "Epoch 7/20\n",
      "142/142 - 31s - loss: 0.6943 - accuracy: 0.6127 - val_loss: 0.6866 - val_accuracy: 0.6111\n",
      "Epoch 8/20\n",
      "142/142 - 32s - loss: 0.7117 - accuracy: 0.5845 - val_loss: 0.6771 - val_accuracy: 0.5833\n",
      "Epoch 9/20\n",
      "142/142 - 33s - loss: 0.6721 - accuracy: 0.5915 - val_loss: 0.6911 - val_accuracy: 0.5556\n",
      "Epoch 10/20\n",
      "142/142 - 32s - loss: 0.6595 - accuracy: 0.5986 - val_loss: 0.6772 - val_accuracy: 0.6111\n",
      "Epoch 11/20\n",
      "142/142 - 28s - loss: 0.6771 - accuracy: 0.6056 - val_loss: 0.6839 - val_accuracy: 0.5833\n",
      "Epoch 12/20\n",
      "142/142 - 28s - loss: 0.6480 - accuracy: 0.6056 - val_loss: 0.7007 - val_accuracy: 0.5833\n",
      "Epoch 13/20\n",
      "142/142 - 29s - loss: 0.6206 - accuracy: 0.6620 - val_loss: 0.6883 - val_accuracy: 0.5833\n",
      "Epoch 14/20\n",
      "142/142 - 28s - loss: 0.6378 - accuracy: 0.6338 - val_loss: 0.6905 - val_accuracy: 0.5556\n",
      "Epoch 15/20\n",
      "142/142 - 28s - loss: 0.6507 - accuracy: 0.6408 - val_loss: 0.6960 - val_accuracy: 0.5556\n",
      "Epoch 16/20\n",
      "142/142 - 29s - loss: 0.6260 - accuracy: 0.6690 - val_loss: 0.6964 - val_accuracy: 0.5556\n",
      "Epoch 17/20\n",
      "142/142 - 28s - loss: 0.6426 - accuracy: 0.6197 - val_loss: 0.6997 - val_accuracy: 0.5278\n",
      "Epoch 18/20\n",
      "142/142 - 28s - loss: 0.6707 - accuracy: 0.6268 - val_loss: 0.6940 - val_accuracy: 0.5278\n",
      "Epoch 19/20\n",
      "142/142 - 28s - loss: 0.5919 - accuracy: 0.6831 - val_loss: 0.6938 - val_accuracy: 0.5556\n",
      "Epoch 20/20\n",
      "142/142 - 28s - loss: 0.5696 - accuracy: 0.7183 - val_loss: 0.7099 - val_accuracy: 0.5556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.91      0.71        22\n",
      "           1       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.56        36\n",
      "   macro avg       0.29      0.45      0.36        36\n",
      "weighted avg       0.36      0.56      0.44        36\n",
      "\n",
      "[[20  2]\n",
      " [14  0]]\n",
      "precision:  0.29411764705882354\n",
      "accuracy:  0.5555555555555556\n",
      "F1 score:  0.35714285714285715\n",
      "recall:  0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "model.fit([X_train, train_metadata], y_train,\n",
    "          validation_data=([X_test, test_metadata], y_test),\n",
    "          epochs=45,\n",
    "          batch_size=100,\n",
    "          verbose=2)\n",
    "predicted = model.predict([X_test, test_metadata])\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"precision: \", str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"accuracy: \", str(metrics.accuracy_score(y_test, predicted)))\n",
    "print(\"F1 score: \", str(metrics.f1_score(y_test, predicted, average='macro')))\n",
    "print(\"recall: \", str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}