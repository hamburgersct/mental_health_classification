{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from textaugment import EDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout, Dense,Input,Embedding,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "columns = ['学号', '性别', '生源地', '总分', '幻觉、妄想症状', '自杀意图', '焦虑指标总分', '抑郁指标总分', '偏执指标总分', '自卑指标总分',\n",
    "               '敏感指标总分', '社交恐惧指标总分', '躯体化指标总分', '依赖指标总分', '敌对攻击指标总分', '冲动指标总分', '强迫指标总分',\n",
    "               '网络成瘾指标总分', '自伤行为指标总分', '进食问题指标总分', '睡眠困扰指标总分', '学校适应困难指标总分', '人际关系困扰指标总分',\n",
    "               '学业压力指标总分', '就业压力指标总分', '恋爱困扰指标总分']\n",
    "data = pd.read_csv('student_data.csv', encoding='utf-8')\n",
    "data.drop(columns=columns, inplace=True)\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": "0    106\n1     88\n2     75\n3     65\nName: 可能问题, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augment training data\n",
    "train_class_0 = data[data['可能问题']==0]\n",
    "train_class_1 = data[data['可能问题']==1]\n",
    "train_class_2 = data[data['可能问题']==2]\n",
    "train_class_3 = data[data['可能问题']==3]\n",
    "\n",
    "t = EDA(random_state=8)\n",
    "new_text_1 = [t.random_swap(t.random_insertion(t.random_deletion(t.synonym_replacement(sent), p=0.4))) for sent in train_class_1['text']]\n",
    "new_text_2 = [t.random_swap(t.random_insertion(t.random_deletion(t.synonym_replacement(sent), p=0.4))) for sent in train_class_2['text']]\n",
    "new_text_3 = [t.random_swap(t.random_insertion(t.random_deletion(t.synonym_replacement(sent), p=0.4))) for sent in train_class_3['text']]\n",
    "\n",
    "new_class_1 = train_class_1\n",
    "new_class_1['text'] = new_text_1\n",
    "train_1 = pd.concat([train_class_1, new_class_1])\n",
    "\n",
    "new_class_2 = train_class_2\n",
    "new_class_2['text'] = new_text_2\n",
    "train_2 = pd.concat([train_class_2, new_class_2, new_class_2, new_class_2, train_class_2])\n",
    "\n",
    "new_class_3 = train_class_3\n",
    "new_class_3['text'] = new_text_3\n",
    "train_3 = pd.concat([train_class_3, new_class_3, new_class_3, new_class_3, train_class_3])\n",
    "\n",
    "train = pd.concat([train_class_0, train_1, train_2, train_3])\n",
    "train['可能问题'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "370"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = np.array(pd.concat([train['text'], test_df['text']]))\n",
    "len(texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\keras_preprocessing\\text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "data": {
      "text/plain": "370"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5000\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "len(sequences)\n",
    "# sequences即将每个句子中的每个单词使用词典序表示的形式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6023 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "labels = to_categorical(np.array(pd.concat([train['可能问题'], test_df['可能问题']])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (370, 500)\n",
      "Shape of label tensor: (370, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences in X_train:  334\n",
      "number of sequences in X_test:  36\n",
      "334\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "X_train = data[:len(train)]\n",
    "X_test = data[len(train):]\n",
    "y_train = np.array(train['可能问题'])\n",
    "y_test = np.array(test_df['可能问题'])\n",
    "\n",
    "print('number of sequences in X_train: ', len(X_train))\n",
    "print('number of sequences in X_test: ', len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Preparing the Embedding Layer\n",
    "embedding_index = {}\n",
    "f = open('./glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embedding_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# embedding matrix\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[index]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[index])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[index] = embedding_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Load embedding matrix into an Embedding Layer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "convs = []\n",
    "filter_sizes = []\n",
    "layer = 3\n",
    "for fl in range(0,layer):\n",
    "    filter_sizes.append((fl+2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "node = 128\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# emb = Reshape((MAX_SEQUENCE_LENGTH,10,10), input_shape=(MAX_SEQUENCE_LENGTH,100))(embedded_sequences)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = keras.layers.Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "    l_pool = keras.layers.MaxPooling1D(3)(l_conv)\n",
    "    # l_pool = Dropout(0.25)(l_pool)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = keras.layers.Concatenate(axis=1)(convs)\n",
    "l_cov1 = keras.layers.Conv1D(node, 5, activation='relu')(l_merge)\n",
    "l_pool1 = keras.layers.MaxPooling1D(3)(l_cov1)\n",
    "l_cov2 = keras.layers.Conv1D(node, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = keras.layers.MaxPooling1D(20)(l_cov2)\n",
    "l_cov2 = Dropout(dropout)(l_pool2)\n",
    "l_flat = Flatten()(l_cov2)\n",
    "# l_dense = Dense(1024, activation='relu')(l_flat)\n",
    "# l_dense = Dropout(dropout)(l_dense)\n",
    "l_dense = Dense(512, activation='relu')(l_flat)\n",
    "l_dense = Dropout(dropout)(l_dense)\n",
    "l_dense = Dense(128, activation='relu')(l_dense)\n",
    "l_dense = Dropout(dropout)(l_dense)\n",
    "\n",
    "preds = Dense(4, activation='softmax')(l_dense)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, \"CNN_augmented_model.png\", show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 334 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "334/334 - 2s - loss: 1.5905 - accuracy: 0.3054 - val_loss: 1.0785 - val_accuracy: 0.6111\n",
      "Epoch 2/20\n",
      "334/334 - 2s - loss: 1.3171 - accuracy: 0.3802 - val_loss: 0.9901 - val_accuracy: 0.6111\n",
      "Epoch 3/20\n",
      "334/334 - 2s - loss: 1.2918 - accuracy: 0.4251 - val_loss: 0.9744 - val_accuracy: 0.6944\n",
      "Epoch 4/20\n",
      "334/334 - 2s - loss: 1.1997 - accuracy: 0.4641 - val_loss: 0.9196 - val_accuracy: 0.6944\n",
      "Epoch 5/20\n",
      "334/334 - 2s - loss: 1.1964 - accuracy: 0.4790 - val_loss: 0.9302 - val_accuracy: 0.6944\n",
      "Epoch 6/20\n",
      "334/334 - 2s - loss: 1.1124 - accuracy: 0.5329 - val_loss: 0.9408 - val_accuracy: 0.6944\n",
      "Epoch 7/20\n",
      "334/334 - 2s - loss: 1.1405 - accuracy: 0.5090 - val_loss: 0.9599 - val_accuracy: 0.6111\n",
      "Epoch 8/20\n",
      "334/334 - 2s - loss: 1.0869 - accuracy: 0.5269 - val_loss: 0.9137 - val_accuracy: 0.6389\n",
      "Epoch 9/20\n",
      "334/334 - 2s - loss: 1.0589 - accuracy: 0.5479 - val_loss: 0.8894 - val_accuracy: 0.6111\n",
      "Epoch 10/20\n",
      "334/334 - 2s - loss: 1.0270 - accuracy: 0.5299 - val_loss: 0.8669 - val_accuracy: 0.6944\n",
      "Epoch 11/20\n",
      "334/334 - 2s - loss: 0.9720 - accuracy: 0.5509 - val_loss: 0.9323 - val_accuracy: 0.6389\n",
      "Epoch 12/20\n",
      "334/334 - 2s - loss: 0.8831 - accuracy: 0.6108 - val_loss: 0.9027 - val_accuracy: 0.6944\n",
      "Epoch 13/20\n",
      "334/334 - 2s - loss: 0.8271 - accuracy: 0.6796 - val_loss: 0.8856 - val_accuracy: 0.7222\n",
      "Epoch 14/20\n",
      "334/334 - 2s - loss: 0.8138 - accuracy: 0.6617 - val_loss: 0.9462 - val_accuracy: 0.6944\n",
      "Epoch 15/20\n",
      "334/334 - 2s - loss: 0.7037 - accuracy: 0.7126 - val_loss: 0.9696 - val_accuracy: 0.7222\n",
      "Epoch 16/20\n",
      "334/334 - 2s - loss: 0.6055 - accuracy: 0.7874 - val_loss: 1.0065 - val_accuracy: 0.7222\n",
      "Epoch 17/20\n",
      "334/334 - 2s - loss: 0.5026 - accuracy: 0.8623 - val_loss: 1.3060 - val_accuracy: 0.6944\n",
      "Epoch 18/20\n",
      "334/334 - 2s - loss: 0.3684 - accuracy: 0.9042 - val_loss: 1.3535 - val_accuracy: 0.7222\n",
      "Epoch 19/20\n",
      "334/334 - 2s - loss: 0.2919 - accuracy: 0.9281 - val_loss: 1.3675 - val_accuracy: 0.7222\n",
      "Epoch 20/20\n",
      "334/334 - 2s - loss: 0.1797 - accuracy: 0.9701 - val_loss: 1.5742 - val_accuracy: 0.7222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.81        22\n",
      "           1       1.00      0.31      0.47        13\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.72        36\n",
      "   macro avg       0.56      0.44      0.43        36\n",
      "weighted avg       0.78      0.72      0.67        36\n",
      "\n",
      "[[22  0  0]\n",
      " [ 9  4  0]\n",
      " [ 1  0  0]]\n",
      "precision:  0.5625\n",
      "accuracy:  0.7222222222222222\n",
      "F1 score:  0.42846768336964414\n",
      "recall:  0.4358974358974359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=20,\n",
    "          batch_size=100,\n",
    "          verbose=2)\n",
    "predicted = model.predict(X_test)\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"precision: \", str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"accuracy: \", str(metrics.accuracy_score(y_test, predicted)))\n",
    "print(\"F1 score: \", str(metrics.f1_score(y_test, predicted, average='macro')))\n",
    "print(\"recall: \", str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nlp_env",
   "language": "python",
   "display_name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}