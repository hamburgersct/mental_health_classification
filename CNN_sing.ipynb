{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout, Dense,Input,Embedding,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "columns = ['学号', '性别', '生源地', '总分', '幻觉、妄想症状', '自杀意图', '焦虑指标总分', '抑郁指标总分', '偏执指标总分', '自卑指标总分',\n",
    "               '敏感指标总分', '社交恐惧指标总分', '躯体化指标总分', '依赖指标总分', '敌对攻击指标总分', '冲动指标总分', '强迫指标总分',\n",
    "               '网络成瘾指标总分', '自伤行为指标总分', '进食问题指标总分', '睡眠困扰指标总分', '学校适应困难指标总分', '人际关系困扰指标总分',\n",
    "               '学业压力指标总分', '就业压力指标总分', '恋爱困扰指标总分']\n",
    "data = pd.read_csv('student_data.csv', encoding='utf-8')\n",
    "data.drop(columns=columns, inplace=True)\n",
    "# data.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0    106\n1     72\nName: 可能问题, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['可能问题'] != 0, '可能问题'] = 1\n",
    "data['可能问题'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_metadata shape: (142, 22)\n",
      "test_metadata shape: (36, 22)\n"
     ]
    }
   ],
   "source": [
    "train_metadata = np.array(train_df.iloc[:, 0:-2])\n",
    "test_metadata = np.array(test_df.iloc[:, 0:-2])\n",
    "print('train_metadata shape:', train_metadata.shape)\n",
    "print('test_metadata shape:', test_metadata.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "178"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = np.array(pd.concat([train_df['text'], test_df['text']]))\n",
    "len(texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\keras_preprocessing\\text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "data": {
      "text/plain": "178"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5000\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "len(sequences)\n",
    "# sequences即将每个句子中的每个单词使用词典序表示的形式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6375 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "labels = to_categorical(np.array(pd.concat([train_df['可能问题'], test_df['可能问题']])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (178, 500)\n",
      "Shape of label tensor: (178, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences in X_train:  142\n",
      "number of sequences in X_test:  36\n",
      "142\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "X_train = data[:len(train_df)]\n",
    "X_test = data[len(train_df):]\n",
    "y_train = np.array(train_df['可能问题'])\n",
    "y_test = np.array(test_df['可能问题'])\n",
    "\n",
    "print('number of sequences in X_train: ', len(X_train))\n",
    "print('number of sequences in X_test: ', len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Preparing the Embedding Layer\n",
    "embedding_index = {}\n",
    "f = open('./glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embedding_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# embedding matrix\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[index]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[index])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[index] = embedding_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Load embedding matrix into an Embedding Layer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "convs = []\n",
    "filter_sizes = []\n",
    "layer = 3\n",
    "for fl in range(0,layer):\n",
    "    filter_sizes.append((fl+2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "node = 128\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "metadata_input = Input(shape=(train_metadata.shape[1],), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# emb = Reshape((MAX_SEQUENCE_LENGTH,10,10), input_shape=(MAX_SEQUENCE_LENGTH,100))(embedded_sequences)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = keras.layers.Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "    l_pool = keras.layers.MaxPooling1D(3)(l_conv)\n",
    "    # l_pool = Dropout(0.25)(l_pool)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = keras.layers.Concatenate(axis=1)(convs)\n",
    "l_cov1 = keras.layers.Conv1D(node, 5, activation='relu')(l_merge)\n",
    "l_pool1 = keras.layers.MaxPooling1D(3)(l_cov1)\n",
    "l_cov2 = keras.layers.Conv1D(node, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = keras.layers.MaxPooling1D(20)(l_cov2)\n",
    "l_cov2 = Dropout(dropout)(l_pool2)\n",
    "l_flat = Flatten()(l_cov2)\n",
    "l_dense = Dense(512, activation='relu')(l_flat)\n",
    "l_dense = Dropout(dropout)(l_dense)\n",
    "l_dense = Dense(256, activation='relu')(l_dense)\n",
    "l_dense = Dropout(dropout)(l_dense)\n",
    "l_dense = Dense(128, activation='relu')(l_dense)\n",
    "l_dense = Dropout(dropout)(l_dense)\n",
    "\n",
    "r_dense = keras.layers.Dense(256, activation='relu')(metadata_input)\n",
    "r_dense = keras.layers.Dropout(dropout)(r_dense)\n",
    "r_dense = keras.layers.Dense(128, activation='relu')(r_dense)\n",
    "r_dense = keras.layers.Dropout(dropout)(r_dense)\n",
    "\n",
    "c_merge = keras.layers.Concatenate(axis=1)([l_dense, r_dense])\n",
    "# c_flat = Flatten()(c_merge)\n",
    "preds = Dense(4, activation='softmax')(c_merge)\n",
    "model = Model([sequence_input, metadata_input], preds)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, \"CNN_metadata_model.png\", show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/50\n",
      "142/142 - 2s - loss: 1.7653 - accuracy: 0.1901 - val_loss: 0.8409 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "142/142 - 1s - loss: 0.8504 - accuracy: 0.6197 - val_loss: 0.6612 - val_accuracy: 0.7778\n",
      "Epoch 3/50\n",
      "142/142 - 1s - loss: 0.6416 - accuracy: 0.6761 - val_loss: 0.5777 - val_accuracy: 0.7500\n",
      "Epoch 4/50\n",
      "142/142 - 1s - loss: 0.6615 - accuracy: 0.7394 - val_loss: 0.5608 - val_accuracy: 0.7778\n",
      "Epoch 5/50\n",
      "142/142 - 1s - loss: 0.7496 - accuracy: 0.7183 - val_loss: 0.5441 - val_accuracy: 0.7222\n",
      "Epoch 6/50\n",
      "142/142 - 1s - loss: 0.6147 - accuracy: 0.7394 - val_loss: 0.5650 - val_accuracy: 0.6667\n",
      "Epoch 7/50\n",
      "142/142 - 1s - loss: 0.4822 - accuracy: 0.7817 - val_loss: 0.6097 - val_accuracy: 0.6944\n",
      "Epoch 8/50\n",
      "142/142 - 1s - loss: 0.4203 - accuracy: 0.8310 - val_loss: 0.6337 - val_accuracy: 0.6944\n",
      "Epoch 9/50\n",
      "142/142 - 1s - loss: 0.4030 - accuracy: 0.8169 - val_loss: 0.6283 - val_accuracy: 0.6944\n",
      "Epoch 10/50\n",
      "142/142 - 1s - loss: 0.3643 - accuracy: 0.8451 - val_loss: 0.6290 - val_accuracy: 0.6944\n",
      "Epoch 11/50\n",
      "142/142 - 1s - loss: 0.3321 - accuracy: 0.8592 - val_loss: 0.6454 - val_accuracy: 0.6944\n",
      "Epoch 12/50\n",
      "142/142 - 1s - loss: 0.3036 - accuracy: 0.8732 - val_loss: 0.6699 - val_accuracy: 0.6944\n",
      "Epoch 13/50\n",
      "142/142 - 1s - loss: 0.3058 - accuracy: 0.8521 - val_loss: 0.6819 - val_accuracy: 0.6944\n",
      "Epoch 14/50\n",
      "142/142 - 1s - loss: 0.3087 - accuracy: 0.8803 - val_loss: 0.6775 - val_accuracy: 0.6944\n",
      "Epoch 15/50\n",
      "142/142 - 1s - loss: 0.2717 - accuracy: 0.8873 - val_loss: 0.6681 - val_accuracy: 0.6944\n",
      "Epoch 16/50\n",
      "142/142 - 1s - loss: 0.2771 - accuracy: 0.8803 - val_loss: 0.6750 - val_accuracy: 0.6944\n",
      "Epoch 17/50\n",
      "142/142 - 1s - loss: 0.2728 - accuracy: 0.8521 - val_loss: 0.7047 - val_accuracy: 0.6944\n",
      "Epoch 18/50\n",
      "142/142 - 1s - loss: 0.3190 - accuracy: 0.8310 - val_loss: 0.7415 - val_accuracy: 0.6944\n",
      "Epoch 19/50\n",
      "142/142 - 1s - loss: 0.2775 - accuracy: 0.8239 - val_loss: 0.7693 - val_accuracy: 0.6944\n",
      "Epoch 20/50\n",
      "142/142 - 1s - loss: 0.3676 - accuracy: 0.8169 - val_loss: 0.7839 - val_accuracy: 0.6111\n",
      "Epoch 21/50\n",
      "142/142 - 1s - loss: 0.2715 - accuracy: 0.8592 - val_loss: 0.7561 - val_accuracy: 0.6944\n",
      "Epoch 22/50\n",
      "142/142 - 1s - loss: 0.2720 - accuracy: 0.8803 - val_loss: 0.7019 - val_accuracy: 0.6944\n",
      "Epoch 23/50\n",
      "142/142 - 1s - loss: 0.2162 - accuracy: 0.9085 - val_loss: 0.6521 - val_accuracy: 0.6944\n",
      "Epoch 24/50\n",
      "142/142 - 1s - loss: 0.2757 - accuracy: 0.8803 - val_loss: 0.6302 - val_accuracy: 0.6389\n",
      "Epoch 25/50\n",
      "142/142 - 1s - loss: 0.2768 - accuracy: 0.8803 - val_loss: 0.6219 - val_accuracy: 0.6389\n",
      "Epoch 26/50\n",
      "142/142 - 1s - loss: 0.2410 - accuracy: 0.9014 - val_loss: 0.6222 - val_accuracy: 0.6389\n",
      "Epoch 27/50\n",
      "142/142 - 1s - loss: 0.1990 - accuracy: 0.9437 - val_loss: 0.6425 - val_accuracy: 0.6667\n",
      "Epoch 28/50\n",
      "142/142 - 1s - loss: 0.2553 - accuracy: 0.8732 - val_loss: 0.7082 - val_accuracy: 0.6944\n",
      "Epoch 29/50\n",
      "142/142 - 1s - loss: 0.2169 - accuracy: 0.9225 - val_loss: 0.7754 - val_accuracy: 0.6111\n",
      "Epoch 30/50\n",
      "142/142 - 1s - loss: 0.2575 - accuracy: 0.8592 - val_loss: 0.8189 - val_accuracy: 0.6111\n",
      "Epoch 31/50\n",
      "142/142 - 1s - loss: 0.2772 - accuracy: 0.8873 - val_loss: 0.8274 - val_accuracy: 0.6111\n",
      "Epoch 32/50\n",
      "142/142 - 1s - loss: 0.2689 - accuracy: 0.8803 - val_loss: 0.8086 - val_accuracy: 0.6111\n",
      "Epoch 33/50\n",
      "142/142 - 1s - loss: 0.2214 - accuracy: 0.8944 - val_loss: 0.7683 - val_accuracy: 0.6389\n",
      "Epoch 34/50\n",
      "142/142 - 1s - loss: 0.2243 - accuracy: 0.8803 - val_loss: 0.7187 - val_accuracy: 0.6667\n",
      "Epoch 35/50\n",
      "142/142 - 1s - loss: 0.2172 - accuracy: 0.8944 - val_loss: 0.6831 - val_accuracy: 0.6667\n",
      "Epoch 36/50\n",
      "142/142 - 1s - loss: 0.1734 - accuracy: 0.9155 - val_loss: 0.6558 - val_accuracy: 0.6944\n",
      "Epoch 37/50\n",
      "142/142 - 1s - loss: 0.2277 - accuracy: 0.8803 - val_loss: 0.6584 - val_accuracy: 0.6944\n",
      "Epoch 38/50\n",
      "142/142 - 1s - loss: 0.2189 - accuracy: 0.9085 - val_loss: 0.6832 - val_accuracy: 0.6667\n",
      "Epoch 39/50\n",
      "142/142 - 1s - loss: 0.2196 - accuracy: 0.8873 - val_loss: 0.7012 - val_accuracy: 0.6667\n",
      "Epoch 40/50\n",
      "142/142 - 1s - loss: 0.2090 - accuracy: 0.9014 - val_loss: 0.7055 - val_accuracy: 0.6667\n",
      "Epoch 41/50\n",
      "142/142 - 1s - loss: 0.1714 - accuracy: 0.9085 - val_loss: 0.6953 - val_accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "142/142 - 1s - loss: 0.2007 - accuracy: 0.9085 - val_loss: 0.6904 - val_accuracy: 0.6667\n",
      "Epoch 43/50\n",
      "142/142 - 1s - loss: 0.1746 - accuracy: 0.9225 - val_loss: 0.6954 - val_accuracy: 0.6667\n",
      "Epoch 44/50\n",
      "142/142 - 1s - loss: 0.1698 - accuracy: 0.9155 - val_loss: 0.6980 - val_accuracy: 0.6667\n",
      "Epoch 45/50\n",
      "142/142 - 1s - loss: 0.1816 - accuracy: 0.9155 - val_loss: 0.7056 - val_accuracy: 0.6667\n",
      "Epoch 46/50\n",
      "142/142 - 1s - loss: 0.1968 - accuracy: 0.9296 - val_loss: 0.7304 - val_accuracy: 0.6667\n",
      "Epoch 47/50\n",
      "142/142 - 1s - loss: 0.1726 - accuracy: 0.9296 - val_loss: 0.7448 - val_accuracy: 0.6667\n",
      "Epoch 48/50\n",
      "142/142 - 1s - loss: 0.1776 - accuracy: 0.9296 - val_loss: 0.7416 - val_accuracy: 0.6667\n",
      "Epoch 49/50\n",
      "142/142 - 1s - loss: 0.2156 - accuracy: 0.9085 - val_loss: 0.7054 - val_accuracy: 0.6389\n",
      "Epoch 50/50\n",
      "142/142 - 1s - loss: 0.1547 - accuracy: 0.9507 - val_loss: 0.6900 - val_accuracy: 0.7222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77        22\n",
      "           1       0.64      0.64      0.64        14\n",
      "\n",
      "    accuracy                           0.72        36\n",
      "   macro avg       0.71      0.71      0.71        36\n",
      "weighted avg       0.72      0.72      0.72        36\n",
      "\n",
      "[[17  5]\n",
      " [ 5  9]]\n",
      "precision:  0.7077922077922079\n",
      "accuracy:  0.7222222222222222\n",
      "F1 score:  0.7077922077922079\n",
      "recall:  0.7077922077922079\n"
     ]
    }
   ],
   "source": [
    "model.fit([X_train, train_metadata], y_train,\n",
    "          validation_data=([X_test, test_metadata], y_test),\n",
    "          epochs=50,\n",
    "          batch_size=128,\n",
    "          verbose=2)\n",
    "predicted = model.predict([X_test, test_metadata])\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"precision: \", str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"accuracy: \", str(metrics.accuracy_score(y_test, predicted)))\n",
    "print(\"F1 score: \", str(metrics.f1_score(y_test, predicted, average='macro')))\n",
    "print(\"recall: \", str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}