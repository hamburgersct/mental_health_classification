{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout, Dense,Input,Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "from textaugment import EDA\n",
    "columns = ['学号', '性别', '生源地', '总分', '幻觉、妄想症状', '自杀意图', '焦虑指标总分', '抑郁指标总分', '偏执指标总分', '自卑指标总分',\n",
    "               '敏感指标总分', '社交恐惧指标总分', '躯体化指标总分', '依赖指标总分', '敌对攻击指标总分', '冲动指标总分', '强迫指标总分',\n",
    "               '网络成瘾指标总分', '自伤行为指标总分', '进食问题指标总分', '睡眠困扰指标总分', '学校适应困难指标总分', '人际关系困扰指标总分',\n",
    "               '学业压力指标总分', '就业压力指标总分', '恋爱困扰指标总分']\n",
    "data = pd.read_csv('student_data.csv', encoding='utf-8')\n",
    "data.drop(columns=columns, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "0    106\n1     72\nName: 可能问题, dtype: int64"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['可能问题'] != 0, '可能问题'] = 1\n",
    "data['可能问题'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_metadata shape: (142, 22)\n",
      "test_metadata shape: (36, 22)\n"
     ]
    }
   ],
   "source": [
    "train_metadata = np.array(train_df.iloc[:, 0:-2])\n",
    "test_metadata = np.array(test_df.iloc[:, 0:-2])\n",
    "print('train_metadata shape:', train_metadata.shape)\n",
    "print('test_metadata shape:', test_metadata.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "178"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = np.array(pd.concat([train_df['text'], test_df['text']]))\n",
    "len(texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\keras_preprocessing\\text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "data": {
      "text/plain": "178"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5000\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "len(sequences)\n",
    "# sequences即将每个句子中的每个单词使用词典序表示的形式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6375 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "labels = to_categorical(np.array(pd.concat([train_df['可能问题'], test_df['可能问题']])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (178, 500)\n",
      "Shape of label tensor: (178, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences in X_train:  142\n",
      "number of sequences in X_test:  36\n",
      "142\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "X_train = data[:len(train_df)]\n",
    "X_test = data[len(train_df):]\n",
    "y_train = np.array(train_df['可能问题'])\n",
    "y_test = np.array(test_df['可能问题'])\n",
    "\n",
    "print('number of sequences in X_train: ', len(X_train))\n",
    "print('number of sequences in X_test: ', len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Preparing the Embedding Layer\n",
    "embedding_index = {}\n",
    "f = open('./glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embedding_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# embedding matrix\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[index]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[index])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[index] = embedding_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "kernel_size = 5\n",
    "filters = 128\n",
    "pool_size = 3\n",
    "gru_node = 128\n",
    "dropout = 0.5\n",
    "# Load embedding matrix into an Embedding Layer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "convs = []\n",
    "filter_sizes = []\n",
    "layer = 3\n",
    "for fl in range(0,layer):\n",
    "    filter_sizes.append((fl+2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "metadata_input = Input(shape=(train_metadata.shape[1],), dtype='float32')\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = keras.layers.Conv1D(128, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "    l_pool = keras.layers.MaxPooling1D(3)(l_conv)\n",
    "    # l_pool = Dropout(0.25)(l_pool)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = keras.layers.Concatenate(axis=1)(convs)\n",
    "\n",
    "# l_model = keras.layers.Dropout(0.25)(embedded_sequences)\n",
    "l_model = keras.layers.Conv1D(filters, kernel_size, activation='relu')(l_merge)\n",
    "l_model = keras.layers.MaxPooling1D(pool_size)(l_model)\n",
    "l_model = keras.layers.Dropout(0.5)(l_model)\n",
    "l_model = keras.layers.Conv1D(filters, kernel_size, activation='relu')(l_model)\n",
    "l_model = keras.layers.MaxPooling1D(pool_size)(l_model)\n",
    "l_model = keras.layers.Dropout(0.5)(l_model)\n",
    "l_model = Bidirectional(keras.layers.LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))(l_model)\n",
    "l_model = Dropout(dropout)(l_model)\n",
    "l_model = Bidirectional(keras.layers.LSTM(gru_node, recurrent_dropout=0.2))(l_model)\n",
    "\n",
    "r_dense = keras.layers.Dense(256, activation='relu')(metadata_input)\n",
    "r_dense = keras.layers.Dropout(dropout)(r_dense)\n",
    "r_dense = keras.layers.Dense(128, activation='relu')(r_dense)\n",
    "# r_dense = keras.layers.Dropout(dropout)(r_dense)\n",
    "\n",
    "c_merge = keras.layers.Concatenate(axis=1)([l_model, r_dense])\n",
    "c_dense = keras.layers.Dense(1024, activation='relu')(c_merge)\n",
    "preds = keras.layers.Dense(4)(c_dense)\n",
    "preds = keras.layers.Activation('softmax')(preds)\n",
    "\n",
    "model = Model([sequence_input, metadata_input], preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, \"HybrNN_multi-input_model.png\", show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/25\n",
      "142/142 - 10s - loss: 1.2568 - accuracy: 0.2535 - val_loss: 0.6640 - val_accuracy: 0.7222\n",
      "Epoch 2/25\n",
      "142/142 - 4s - loss: 0.5675 - accuracy: 0.7535 - val_loss: 0.6046 - val_accuracy: 0.6111\n",
      "Epoch 3/25\n",
      "142/142 - 4s - loss: 0.4497 - accuracy: 0.7606 - val_loss: 0.5214 - val_accuracy: 0.7500\n",
      "Epoch 4/25\n",
      "142/142 - 4s - loss: 0.3607 - accuracy: 0.8169 - val_loss: 0.5281 - val_accuracy: 0.6944\n",
      "Epoch 5/25\n",
      "142/142 - 5s - loss: 0.2990 - accuracy: 0.8310 - val_loss: 0.5761 - val_accuracy: 0.7778\n",
      "Epoch 6/25\n",
      "142/142 - 4s - loss: 0.3154 - accuracy: 0.8310 - val_loss: 0.6322 - val_accuracy: 0.7222\n",
      "Epoch 7/25\n",
      "142/142 - 5s - loss: 0.2723 - accuracy: 0.8944 - val_loss: 0.6346 - val_accuracy: 0.7500\n",
      "Epoch 8/25\n",
      "142/142 - 5s - loss: 0.2545 - accuracy: 0.8803 - val_loss: 0.5974 - val_accuracy: 0.6944\n",
      "Epoch 9/25\n",
      "142/142 - 5s - loss: 0.2709 - accuracy: 0.8732 - val_loss: 0.6737 - val_accuracy: 0.7778\n",
      "Epoch 10/25\n",
      "142/142 - 5s - loss: 0.2596 - accuracy: 0.8803 - val_loss: 0.5622 - val_accuracy: 0.7500\n",
      "Epoch 11/25\n",
      "142/142 - 5s - loss: 0.2222 - accuracy: 0.8803 - val_loss: 0.6137 - val_accuracy: 0.6667\n",
      "Epoch 12/25\n",
      "142/142 - 5s - loss: 0.2600 - accuracy: 0.8873 - val_loss: 0.5370 - val_accuracy: 0.8333\n",
      "Epoch 13/25\n",
      "142/142 - 5s - loss: 0.1835 - accuracy: 0.9296 - val_loss: 0.6262 - val_accuracy: 0.7778\n",
      "Epoch 14/25\n",
      "142/142 - 5s - loss: 0.2038 - accuracy: 0.9085 - val_loss: 0.6411 - val_accuracy: 0.7778\n",
      "Epoch 15/25\n",
      "142/142 - 4s - loss: 0.1890 - accuracy: 0.9366 - val_loss: 0.5287 - val_accuracy: 0.8056\n",
      "Epoch 16/25\n",
      "142/142 - 5s - loss: 0.1585 - accuracy: 0.9366 - val_loss: 0.5374 - val_accuracy: 0.8056\n",
      "Epoch 17/25\n",
      "142/142 - 4s - loss: 0.1693 - accuracy: 0.9155 - val_loss: 0.5627 - val_accuracy: 0.8056\n",
      "Epoch 18/25\n",
      "142/142 - 5s - loss: 0.1372 - accuracy: 0.9366 - val_loss: 0.5959 - val_accuracy: 0.7778\n",
      "Epoch 19/25\n",
      "142/142 - 5s - loss: 0.1424 - accuracy: 0.9507 - val_loss: 0.6017 - val_accuracy: 0.8056\n",
      "Epoch 20/25\n",
      "142/142 - 5s - loss: 0.1151 - accuracy: 0.9577 - val_loss: 0.6300 - val_accuracy: 0.7778\n",
      "Epoch 21/25\n",
      "142/142 - 5s - loss: 0.1078 - accuracy: 0.9437 - val_loss: 0.6493 - val_accuracy: 0.7778\n",
      "Epoch 22/25\n",
      "142/142 - 5s - loss: 0.0658 - accuracy: 0.9859 - val_loss: 0.6434 - val_accuracy: 0.7778\n",
      "Epoch 23/25\n",
      "142/142 - 5s - loss: 0.0892 - accuracy: 0.9789 - val_loss: 0.6561 - val_accuracy: 0.7778\n",
      "Epoch 24/25\n",
      "142/142 - 5s - loss: 0.1108 - accuracy: 0.9437 - val_loss: 0.6939 - val_accuracy: 0.7500\n",
      "Epoch 25/25\n",
      "142/142 - 5s - loss: 0.0882 - accuracy: 0.9718 - val_loss: 0.7329 - val_accuracy: 0.7500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82        22\n",
      "           1       0.78      0.50      0.61        14\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.83      0.76      0.78        36\n",
      "weighted avg       0.83      0.75      0.78        36\n",
      "\n",
      "[[20  2]\n",
      " [ 7  7]]\n",
      "precision:  0.8333333333333333\n",
      "accuracy:  0.8055555\n",
      "F1 score:  0.776811091393079\n",
      "recall:  0.7565454545454546\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, train_metadata], y_train,\n",
    "          validation_data=([X_test, test_metadata], y_test),\n",
    "          epochs=25,\n",
    "          batch_size=100,\n",
    "          verbose=2)\n",
    "predicted = model.predict([X_test, test_metadata])\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"precision: \", str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"accuracy: \", str(metrics.accuracy_score(y_test, predicted)))\n",
    "print(\"F1 score: \", str(metrics.f1_score(y_test, predicted, average='macro')))\n",
    "print(\"recall: \", str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nlp_env",
   "language": "python",
   "display_name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}