{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   幻觉、妄想症状指标标准分  自杀意图指标标准分  焦虑指标标准分  抑郁指标标准分  偏执指标标准分  自卑指标标准分  敏感指标标准分  \\\n0          2.79       2.38     0.50     3.98     1.48     3.88     2.56   \n1          0.76       1.69     0.50     1.67     0.51     0.90     2.14   \n2         -0.59       1.69    -0.91     1.67    -0.94    -0.59     0.49   \n\n   社交恐惧指标标准分  躯体化指标标准分  依赖指标标准分  ...  自伤行为指标标准分  进食问题指标标准分  睡眠困扰指标标准分  \\\n0       4.05       1.5     3.52  ...       3.04       1.65       2.33   \n1       1.80      -0.7     0.37  ...       0.17       1.65      -0.22   \n2      -0.91       0.4    -0.98  ...      -0.55      -0.28      -1.06   \n\n   学校适应困难指标标准分  人际关系困扰指标标准分  学业压力指标标准分  就业压力指标标准分  恋爱困扰指标标准分  可能问题  \\\n0         1.56         3.28       2.22       1.89      -1.04     3   \n1         3.40         0.88       1.48       1.56       3.26     1   \n2         3.40         0.40       0.75       0.25       0.87     3   \n\n                                                text  \n0  im gansu province zhangye city northwest china...  \n1  right im sitting behind computer screen wonder...  \n2  look back year trampled shallowly always sever...  \n\n[3 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>幻觉、妄想症状指标标准分</th>\n      <th>自杀意图指标标准分</th>\n      <th>焦虑指标标准分</th>\n      <th>抑郁指标标准分</th>\n      <th>偏执指标标准分</th>\n      <th>自卑指标标准分</th>\n      <th>敏感指标标准分</th>\n      <th>社交恐惧指标标准分</th>\n      <th>躯体化指标标准分</th>\n      <th>依赖指标标准分</th>\n      <th>...</th>\n      <th>自伤行为指标标准分</th>\n      <th>进食问题指标标准分</th>\n      <th>睡眠困扰指标标准分</th>\n      <th>学校适应困难指标标准分</th>\n      <th>人际关系困扰指标标准分</th>\n      <th>学业压力指标标准分</th>\n      <th>就业压力指标标准分</th>\n      <th>恋爱困扰指标标准分</th>\n      <th>可能问题</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.79</td>\n      <td>2.38</td>\n      <td>0.50</td>\n      <td>3.98</td>\n      <td>1.48</td>\n      <td>3.88</td>\n      <td>2.56</td>\n      <td>4.05</td>\n      <td>1.5</td>\n      <td>3.52</td>\n      <td>...</td>\n      <td>3.04</td>\n      <td>1.65</td>\n      <td>2.33</td>\n      <td>1.56</td>\n      <td>3.28</td>\n      <td>2.22</td>\n      <td>1.89</td>\n      <td>-1.04</td>\n      <td>3</td>\n      <td>im gansu province zhangye city northwest china...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.76</td>\n      <td>1.69</td>\n      <td>0.50</td>\n      <td>1.67</td>\n      <td>0.51</td>\n      <td>0.90</td>\n      <td>2.14</td>\n      <td>1.80</td>\n      <td>-0.7</td>\n      <td>0.37</td>\n      <td>...</td>\n      <td>0.17</td>\n      <td>1.65</td>\n      <td>-0.22</td>\n      <td>3.40</td>\n      <td>0.88</td>\n      <td>1.48</td>\n      <td>1.56</td>\n      <td>3.26</td>\n      <td>1</td>\n      <td>right im sitting behind computer screen wonder...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.59</td>\n      <td>1.69</td>\n      <td>-0.91</td>\n      <td>1.67</td>\n      <td>-0.94</td>\n      <td>-0.59</td>\n      <td>0.49</td>\n      <td>-0.91</td>\n      <td>0.4</td>\n      <td>-0.98</td>\n      <td>...</td>\n      <td>-0.55</td>\n      <td>-0.28</td>\n      <td>-1.06</td>\n      <td>3.40</td>\n      <td>0.40</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.87</td>\n      <td>3</td>\n      <td>look back year trampled shallowly always sever...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "columns = ['学号', '性别', '生源地', '总分', '幻觉、妄想症状', '自杀意图', '焦虑指标总分', '抑郁指标总分', '偏执指标总分', '自卑指标总分',\n",
    "               '敏感指标总分', '社交恐惧指标总分', '躯体化指标总分', '依赖指标总分', '敌对攻击指标总分', '冲动指标总分', '强迫指标总分',\n",
    "               '网络成瘾指标总分', '自伤行为指标总分', '进食问题指标总分', '睡眠困扰指标总分', '学校适应困难指标总分', '人际关系困扰指标总分',\n",
    "               '学业压力指标总分', '就业压力指标总分', '恋爱困扰指标总分']\n",
    "data = pd.read_csv('student_data.csv', encoding='utf-8')\n",
    "data.drop(columns=columns, inplace=True)\n",
    "data.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0    106\n1     72\nName: 可能问题, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['可能问题'] != 0, '可能问题'] = 1\n",
    "data['可能问题'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0    75\n1    27\n3    11\n2    11\nName: 可能问题, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['可能问题'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_class_0 = train_df[train_df['可能问题']==0]\n",
    "train_class_1 = train_df[train_df['可能问题']==1]\n",
    "train_class_2 = train_df[train_df['可能问题']==2]\n",
    "train_class_3 = train_df[train_df['可能问题']==3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from textaugment import EDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "t = EDA(random_state=8)\n",
    "new_text_1 = [t.random_swap(t.random_insertion(t.random_deletion(t.synonym_replacement(sent), p=0.4))) for sent in train_class_1['text']]\n",
    "new_text_2 = [t.random_swap(t.random_insertion(t.random_deletion(t.synonym_replacement(sent), p=0.4))) for sent in train_class_2['text']]\n",
    "new_text_3 = [t.random_swap(t.random_insertion(t.random_deletion(t.synonym_replacement(sent), p=0.4))) for sent in train_class_3['text']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "new_class_1 = train_class_1\n",
    "new_class_1['text'] = new_text_1\n",
    "train_1 = pd.concat([train_class_1, new_class_1])\n",
    "\n",
    "new_class_2 = train_class_2\n",
    "new_class_2['text'] = new_text_2\n",
    "train_2 = pd.concat([train_class_2, new_class_2])\n",
    "\n",
    "new_class_3 = train_class_3\n",
    "new_class_3['text'] = new_text_3\n",
    "train_3 = pd.concat([train_class_3, new_class_3])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0    75\n1    54\n3    22\n2    22\nName: 可能问题, dtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train_class_0, train_1, train_2, train_3])\n",
    "train['可能问题'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['幻觉、妄想症状指标标准分', '自杀意图指标标准分', '焦虑指标标准分', '抑郁指标标准分', '偏执指标标准分', '自卑指标标准分',\n       '敏感指标标准分', '社交恐惧指标标准分', '躯体化指标标准分', '依赖指标标准分', '敌对攻击指标标准分', '冲动指标标准分',\n       '强迫指标标准分', '网络成瘾指标标准分', '自伤行为指标标准分', '进食问题指标标准分', '睡眠困扰指标标准分',\n       '学校适应困难指标标准分', '人际关系困扰指标标准分', '学业压力指标标准分', '就业压力指标标准分', '恋爱困扰指标标准分',\n       '可能问题', 'text'],\n      dtype='object')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.columns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Create Function Transformer to use Feature Union\n",
    "def get_numeric_data(x):\n",
    "    return np.array(x.iloc[:, 0:-2])\n",
    "\n",
    "\n",
    "def get_text_data(x):\n",
    "    return x['text'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def metadata_LR_fu():\n",
    "\n",
    "    y_train = train['可能问题'].tolist()\n",
    "    y_test = test_df['可能问题'].tolist()\n",
    "\n",
    "    transformer_numeric = FunctionTransformer(get_numeric_data)\n",
    "    transformer_text = FunctionTransformer(get_text_data)\n",
    "\n",
    "    # Create a pipeline to concatenate Tfidf Vector and Numeric data\n",
    "    # Use SVM as classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('metadata', FeatureUnion([\n",
    "            ('numeric_feature', Pipeline([\n",
    "                ('selector', transformer_numeric)\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', transformer_text),\n",
    "                ('vec', TfidfVectorizer())\n",
    "            ]))\n",
    "        ])),\n",
    "        ('clf', LogisticRegression(max_iter=200))\n",
    "    ])\n",
    "\n",
    "    # Grid Search Parameters for SGDClassifer\n",
    "    parameters = {\n",
    "        # 'clf__var_smoothing': (1e-4, 1e-6),\n",
    "        'metadata__text_features__vec__ngram_range': [(1, 2), (1, 3)],\n",
    "        'metadata__text_features__vec__use_idf': [True, False]\n",
    "    }\n",
    "\n",
    "    # Training config\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n",
    "    refit = 'F1'\n",
    "\n",
    "    gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1, cv=kfold, scoring=scoring, refit=refit)\n",
    "    gs_clf.fit(train, y_train)\n",
    "\n",
    "    predicted = gs_clf.predict(test_df)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "    print(metrics.confusion_matrix(y_test, predicted))\n",
    "    print(\"precision: \", str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "    print(\"accuracy: \", str(metrics.accuracy_score(y_test, predicted)))\n",
    "    print(\"F1 score: \", str(metrics.f1_score(y_test, predicted, average='macro')))\n",
    "    print(\"recall: \", str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75        31\n",
      "           1       0.45      0.29      0.36        17\n",
      "           2       0.14      0.25      0.18         4\n",
      "           3       0.33      0.50      0.40         2\n",
      "\n",
      "    accuracy                           0.57        54\n",
      "   macro avg       0.41      0.45      0.42        54\n",
      "weighted avg       0.58      0.57      0.57        54\n",
      "\n",
      "[[24  4  3  0]\n",
      " [ 9  5  2  1]\n",
      " [ 0  2  1  1]\n",
      " [ 0  0  1  1]]\n",
      "precision:  0.4145021645021645\n",
      "accuracy:  0.5740740740740741\n",
      "F1 score:  0.4222402597402598\n",
      "recall:  0.4545777988614801\n"
     ]
    }
   ],
   "source": [
    "metadata_LR_fu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def standard_LR():\n",
    "\n",
    "    X_train = train['text'].tolist()\n",
    "    X_test = test_df['text'].tolist()\n",
    "    y_train = train['可能问题'].tolist()\n",
    "    y_test = test_df['可能问题'].tolist()\n",
    "\n",
    "    pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LogisticRegression(max_iter=200)),\n",
    "                         ])\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "        # 'clf__var_smoothing': (1e-4, 1e-6)\n",
    "    }\n",
    "\n",
    "    # Training config\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n",
    "    refit = 'F1'\n",
    "\n",
    "    gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1, cv=kfold, scoring=scoring, refit=refit)\n",
    "    gs_clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = gs_clf.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "    print(metrics.confusion_matrix(y_test, predicted))\n",
    "    print(\"precision: \", str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "    print(\"accuracy: \", str(metrics.accuracy_score(y_test, predicted)))\n",
    "    print(\"F1 score: \", str(metrics.f1_score(y_test, predicted, average='macro')))\n",
    "    print(\"recall: \", str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73        31\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.57        54\n",
      "   macro avg       0.14      0.25      0.18        54\n",
      "weighted avg       0.33      0.57      0.42        54\n",
      "\n",
      "[[31  0  0  0]\n",
      " [17  0  0  0]\n",
      " [ 4  0  0  0]\n",
      " [ 2  0  0  0]]\n",
      "precision:  0.14351851851851852\n",
      "accuracy:  0.5740740740740741\n",
      "F1 score:  0.1823529411764706\n",
      "recall:  0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hamburger_sct\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "standard_LR()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nlp_env",
   "language": "python",
   "display_name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}